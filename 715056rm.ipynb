{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b71f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student data generator for digital transformation dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Write your student number here\n",
    "STUDENT_number = 715056\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('digital_transformation.csv')\n",
    "np.random.seed(STUDENT_number)\n",
    "noise_vars = ['revenue', 'productivity_score', 'employee_satisfaction', 'customer_satisfaction', 'ceo_experience']\n",
    "for var in noise_vars:\n",
    "    df[var] += np.random.normal(0, df[var].std() * 0.05)\n",
    "\n",
    "# Save unique dataset\n",
    "df.to_csv('digital_transformation_unique.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86bb303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from linearmodels.panel import PanelOLS\n",
    "from scipy.stats import gaussian_kde, f\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import CausalML, fallback if not available\n",
    "try:\n",
    "    from causalml.inference.tree import CausalTreeRegressor\n",
    "    CAUSAL_ML_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è CausalML not available. HTE will fallback.\")\n",
    "    CAUSAL_ML_AVAILABLE = False\n",
    "\n",
    "# Try to import EconML as backup\n",
    "try:\n",
    "    from econml.dml import DMLOrthoForest\n",
    "    ECONML_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ECONML_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "659aaad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables in dataset:\n",
      "['firm_id', 'year', 'industry_type', 'firm_strategy', 'firm_size', 'firm_age', 'ceo_experience', 'ceo_tenure', 'prior_tech_adoption', 'tech_budget_pct', 'digital_transformation_program', 'market_conditions', 'revenue', 'revenue_growth', 'operating_costs', 'rd_spending', 'productivity_score', 'employee_satisfaction', 'customer_satisfaction', 'profit_margin', 'rd_intensity', 'large_firm', 'post_2020']\n",
      "\n",
      "Descriptive Statistics (Numeric):\n",
      "           firm_id        year      firm_size    firm_age  ceo_experience  \\\n",
      "count  3000.000000  3000.00000    3000.000000  3000.00000     3000.000000   \n",
      "mean    250.500000  2020.50000   11203.480333    23.31000       10.222409   \n",
      "std     144.361341     1.70811   30609.496929    10.39724        7.752553   \n",
      "min       1.000000  2018.00000      23.000000     4.00000        2.106009   \n",
      "25%     125.750000  2019.00000    1364.000000    14.00000        4.381009   \n",
      "50%     250.500000  2020.50000    3698.500000    23.00000        7.806009   \n",
      "75%     375.250000  2022.00000   10111.000000    32.00000       13.606009   \n",
      "max     500.000000  2023.00000  480435.000000    43.00000       46.806009   \n",
      "\n",
      "        ceo_tenure  prior_tech_adoption  tech_budget_pct  \\\n",
      "count  3000.000000          3000.000000      3000.000000   \n",
      "mean      4.890000             0.378000         0.085514   \n",
      "std       3.992074             0.484968         0.037018   \n",
      "min       1.000000             0.000000         0.020000   \n",
      "25%       2.100000             0.000000         0.054000   \n",
      "50%       3.600000             0.000000         0.086500   \n",
      "75%       6.525000             1.000000         0.116000   \n",
      "max      24.400000             1.000000         0.150000   \n",
      "\n",
      "       digital_transformation_program  market_conditions  ...  revenue_growth  \\\n",
      "count                     3000.000000        3000.000000  ...     3000.000000   \n",
      "mean                         0.557667          -0.028020  ...        0.101504   \n",
      "std                          0.496746           0.117843  ...        0.129320   \n",
      "min                          0.000000          -0.441000  ...       -0.302300   \n",
      "25%                          0.000000          -0.102000  ...        0.012700   \n",
      "50%                          1.000000          -0.022000  ...        0.103450   \n",
      "75%                          1.000000           0.052000  ...        0.185575   \n",
      "max                          1.000000           0.351000  ...        0.571400   \n",
      "\n",
      "       operating_costs   rd_spending  productivity_score  \\\n",
      "count     3.000000e+03  3.000000e+03         3000.000000   \n",
      "mean      1.081936e+07  7.733135e+05           54.934976   \n",
      "std       3.095708e+07  2.612118e+06           17.169689   \n",
      "min       1.719523e+04  4.124400e+02            0.056276   \n",
      "25%       1.062125e+06  5.619429e+04           43.356276   \n",
      "50%       3.089804e+06  1.844352e+05           54.556276   \n",
      "75%       9.184575e+06  6.220318e+05           66.581276   \n",
      "max       5.934967e+08  6.682606e+07          127.256276   \n",
      "\n",
      "       employee_satisfaction  customer_satisfaction  profit_margin  \\\n",
      "count            3000.000000            3000.000000    3000.000000   \n",
      "mean                3.690545               4.145233       0.253128   \n",
      "std                 0.776583               0.589704       0.054233   \n",
      "min                 1.033572               2.068736       0.105656   \n",
      "25%                 3.163572               3.718736       0.213668   \n",
      "50%                 3.703572               4.168736       0.253563   \n",
      "75%                 4.253572               4.628736       0.292745   \n",
      "max                 5.033572               5.018736       0.395047   \n",
      "\n",
      "       rd_intensity   large_firm    post_2020  \n",
      "count   3000.000000  3000.000000  3000.000000  \n",
      "mean       0.051211     0.500000     0.666667  \n",
      "std        0.024298     0.500083     0.471483  \n",
      "min        0.009749     0.000000     0.000000  \n",
      "25%        0.030394     0.000000     0.000000  \n",
      "50%        0.050745     0.500000     1.000000  \n",
      "75%        0.069883     1.000000     1.000000  \n",
      "max        0.113700     1.000000     1.000000  \n",
      "\n",
      "[8 rows x 21 columns]\n",
      "\n",
      "Descriptive Statistics (Non-Numeric):\n",
      "       industry_type      firm_strategy\n",
      "count           3000               3000\n",
      "unique             8                  5\n",
      "top      Real Estate  Customer Intimacy\n",
      "freq             450                666\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('digital_transformation_unique.csv')\n",
    "\n",
    "# Get variable names\n",
    "print(\"Variables in dataset:\")\n",
    "print(list(df.columns))\n",
    "\n",
    "# Descriptive statistics for numeric variables\n",
    "print(\"\\nDescriptive Statistics (Numeric):\")\n",
    "print(df.describe())\n",
    "\n",
    "# Descriptive statistics for non-numeric variables\n",
    "print(\"\\nDescriptive Statistics (Non-Numeric):\")\n",
    "print(df.select_dtypes(include='object').describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2a5b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Grouped by digital_transformation_program                                                                        \n",
      "                                                                           Missing      Overall            0           1 SMD (0,1) P-Value            Test\n",
      "n                                                                                           500          417          83                                  \n",
      "log_firm_size, mean (SD)                                                         0    8.1 (1.5)    8.0 (1.5)   8.8 (1.4)     0.610  <0.001  Welch‚Äôs T-test\n",
      "firm_age, mean (SD)                                                              0  20.8 (10.3)  20.9 (10.4)  20.3 (9.6)    -0.061   0.602  Welch‚Äôs T-test\n",
      "prior_tech_adoption, n (%) 0                                                         311 (62.2)   251 (60.2)   60 (72.3)     0.258   0.051     Chi-squared\n",
      "                           1                                                         189 (37.8)   166 (39.8)   23 (27.7)                                  \n",
      "tech_budget_pct, mean (SD)                                                       0    0.1 (0.0)    0.1 (0.0)   0.1 (0.0)    -0.178   0.152  Welch‚Äôs T-test\n",
      "ceo_experience, mean (SD)                                                        0   10.2 (7.8)   10.3 (7.9)   9.9 (6.9)    -0.047   0.680  Welch‚Äôs T-test\n",
      "industry_group, n (%)      Healthcare                                                 67 (13.4)    62 (14.9)     5 (6.0)     0.298   0.177     Chi-squared\n",
      "                           Manufacturing                                              74 (14.8)    61 (14.6)   13 (15.7)                                  \n",
      "                           Other                                                     284 (56.8)   234 (56.1)   50 (60.2)                                  \n",
      "                           Real Estate                                                75 (15.0)    60 (14.4)   15 (18.1)                                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Create essential variables for causal analysis\"\"\"\n",
    "    # Feature engineering\n",
    "    df['log_firm_size'] = np.log(df['firm_size'] + 1)\n",
    "    df['post_treatment'] = (df['year'] > 2020).astype(int)\n",
    "    \n",
    "    # Industry simplification\n",
    "    top_industries = df['industry_type'].value_counts().nlargest(3).index\n",
    "    df['industry_group'] = df['industry_type'].where(\n",
    "        df['industry_type'].isin(top_industries), 'Other'\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def assess_covariate_balance(df):\n",
    "    \"\"\"Check pre-treatment comparability of groups\"\"\"\n",
    "    from tableone import TableOne\n",
    "    \n",
    "    covariates = ['log_firm_size', 'firm_age', 'prior_tech_adoption', \n",
    "                 'tech_budget_pct', 'ceo_experience', 'industry_group']\n",
    "    \n",
    "    table = TableOne(df, covariates, groupby='digital_transformation_program',\n",
    "                    pval=True, smd=True, htest_name=True)\n",
    "    return table\n",
    "\n",
    "df = preprocess_data(df)\n",
    "\n",
    "balance_table = assess_covariate_balance(df[df['year'] == 2018])  # Pre-treatment year\n",
    "print(balance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afae17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó MULTICOLLINEARITY ANALYSIS:\n",
      "----------------------------------------\n",
      "Converting categorical variables: ['industry_group']\n",
      "\n",
      "Variance Inflation Factors (Top 10):\n",
      "                       Variable       VIF\n",
      "7          industry_group_Other  2.275772\n",
      "8    industry_group_Real Estate  1.811153\n",
      "6  industry_group_Manufacturing  1.797048\n",
      "3           prior_tech_adoption  1.013640\n",
      "2                ceo_experience  1.011113\n",
      "4               tech_budget_pct  1.009149\n",
      "1                      firm_age  1.006049\n",
      "0                 log_firm_size  1.002426\n",
      "5             market_conditions  1.000971\n",
      "\n",
      "üü¢ No significant multicollinearity detected\n"
     ]
    }
   ],
   "source": [
    "def check_multicollinearity(df, covariates):\n",
    "    \"\"\"Check VIF for multicollinearity with enhanced reporting - FIXED VERSION\"\"\"\n",
    "    print(\"\\nüîó MULTICOLLINEARITY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Prepare data - handle categorical conversion\n",
    "    X = df[covariates].copy()\n",
    "    \n",
    "    # Convert categoricals and ensure FULL numeric conversion\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    if not cat_cols.empty:\n",
    "        print(f\"Converting categorical variables: {list(cat_cols)}\")\n",
    "        X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    # CRITICAL FIX: Force conversion to numeric and drop non-convertible columns\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    non_numeric_cols = X.columns[X.isnull().any()]\n",
    "    if not non_numeric_cols.empty:\n",
    "        print(f\"Removing non-convertible columns: {list(non_numeric_cols)}\")\n",
    "        X = X.drop(columns=non_numeric_cols)\n",
    "    \n",
    "    # Add constant and calculate VIF\n",
    "    X_const = sm.add_constant(X)\n",
    "    vif_data = []\n",
    "    for i, col in enumerate(X_const.columns):\n",
    "        if col != 'const':\n",
    "            vif = variance_inflation_factor(X_const.values.astype(float), i)  # Ensure float type\n",
    "            vif_data.append({'Variable': col, 'VIF': vif})\n",
    "    \n",
    "    vif_df = pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
    "    \n",
    "    # Critical diagnostics (unchanged)\n",
    "    high_vif = vif_df[vif_df['VIF'] > 10]\n",
    "    moderate_vif = vif_df[(vif_df['VIF'] > 5) & (vif_df['VIF'] <= 10)]\n",
    "    \n",
    "    print(\"\\nVariance Inflation Factors (Top 10):\")\n",
    "    print(vif_df.head(10))\n",
    "    \n",
    "    if not high_vif.empty:\n",
    "        print(f\"\\nüî¥ Critical multicollinearity (VIF > 10):\")\n",
    "        print(high_vif)\n",
    "    elif not moderate_vif.empty:\n",
    "        print(f\"\\nüü† Moderate multicollinearity (5 < VIF ‚â§ 10):\")\n",
    "        print(moderate_vif)\n",
    "    else:\n",
    "        print(\"\\nüü¢ No significant multicollinearity detected\")\n",
    "    \n",
    "    return vif_df\n",
    "\n",
    "# Define key covariates (use consistent naming)\n",
    "key_covariates = [\n",
    "    'log_firm_size',\n",
    "    'firm_age',\n",
    "    'ceo_experience',\n",
    "    'prior_tech_adoption',\n",
    "    'tech_budget_pct',\n",
    "    'market_conditions',\n",
    "    'industry_group'  # Will be properly converted\n",
    "]\n",
    "\n",
    "# Re-run analysis\n",
    "vif_results = check_multicollinearity(df, key_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb6a23ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COURSE-ALIGNED CAUSAL ANALYSIS PIPELINE\n",
      "============================================================\n",
      "‚úÖ Dataset loaded successfully\n",
      "Dataset shape: (3000, 23)\n",
      "Estimating treatment propensities for MSM...\n",
      "Using confounders: ['rd_intensity_lag1', 'tech_budget_lag1', 'ceo_exp_lag1', 'productivity_lag1', 'treatment_lag1', 'treatment_lag2', 'firm_age', 'log_firm_size']\n",
      "‚úÖ MSM weights computed: mean=0.813, min=0.470, max=8.514\n",
      "üìä Analyzing outcomes: ['productivity_score', 'employee_satisfaction']\n",
      "\n",
      "==================== ANALYZING: PRODUCTIVITY_SCORE ====================\n",
      "\n",
      "1Ô∏è‚É£ Running MSM/IPTW Analysis...\n",
      "Running MSM analysis for productivity_score...\n",
      "\n",
      "2Ô∏è‚É£ Testing Parallel Trends...\n",
      "Testing parallel trends for productivity_score...\n",
      "Parallel trends test: coef=0.5168, p-value=0.0084\n",
      "‚ö†Ô∏è Possible violation\n",
      "\n",
      "3Ô∏è‚É£ Running Subgroup Analysis...\n",
      "Running subgroup analysis by firm_size_bin...\n",
      "Running subgroup analysis by prior_tech_adoption...\n",
      "\n",
      "4Ô∏è‚É£ Running Moderation Analysis...\n",
      "Running moderation analysis with log_firm_size...\n",
      "\n",
      "5Ô∏è‚É£ Running Causal Tree Analysis...\n",
      "Running causal tree analysis for 'productivity_score'...\n",
      "‚úÖ Causal tree analysis completed successfully.\n",
      "\n",
      "==================== ANALYZING: EMPLOYEE_SATISFACTION ====================\n",
      "\n",
      "1Ô∏è‚É£ Running MSM/IPTW Analysis...\n",
      "Running MSM analysis for employee_satisfaction...\n",
      "\n",
      "2Ô∏è‚É£ Testing Parallel Trends...\n",
      "Testing parallel trends for employee_satisfaction...\n",
      "Parallel trends test: coef=0.2602, p-value=0.2259\n",
      "‚úÖ Parallel trends holds\n",
      "\n",
      "3Ô∏è‚É£ Running Subgroup Analysis...\n",
      "Running subgroup analysis by firm_size_bin...\n",
      "Running subgroup analysis by prior_tech_adoption...\n",
      "\n",
      "4Ô∏è‚É£ Running Moderation Analysis...\n",
      "Running moderation analysis with log_firm_size...\n",
      "\n",
      "5Ô∏è‚É£ Running Causal Tree Analysis...\n",
      "Running causal tree analysis for 'employee_satisfaction'...\n",
      "‚úÖ Causal tree analysis completed successfully.\n",
      "\n",
      "üìù Generating comprehensive report...\n",
      "‚úÖ Analysis complete! Results saved:\n",
      "  - course_aligned_causal_results.txt\n",
      "  - ate_comparison.png\n",
      "  - Outcome-specific visualizations\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# DATA PREPARATION\n",
    "# ======================\n",
    "\n",
    "def load_and_validate_data(path='digital_transformation_unique.csv'):\n",
    "    \"\"\"Load and validate digital transformation dataset\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(\"‚úÖ Dataset loaded successfully\")\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Dataset file not found. Please ensure the CSV exists.\")\n",
    "        return None\n",
    "\n",
    "def winsorize_series(series, lower=0.01, upper=0.99):\n",
    "    \"\"\"Winsorize series to handle outliers\"\"\"\n",
    "    lower_bound = series.quantile(lower)\n",
    "    upper_bound = series.quantile(upper)\n",
    "    return np.clip(series, lower_bound, upper_bound)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Feature engineering with proper time-varying covariates for MSM\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Basic feature engineering\n",
    "    df['log_firm_size'] = np.log(df.get('firm_size', 0) + 1)\n",
    "    df['firm_size_bin'] = pd.cut(df['log_firm_size'], bins=[0,7,9,11,15], \n",
    "                                labels=['Micro','Small','Medium','Large'])\n",
    "    \n",
    "    # Sort for proper lagging\n",
    "    df = df.sort_values(['firm_id','year'])\n",
    "    \n",
    "    # Create lagged confounders for MSM (Session 7 approach)\n",
    "    df['rd_intensity_lag1'] = df.groupby('firm_id')['rd_intensity'].shift(1).fillna(0)\n",
    "    df['tech_budget_lag1'] = df.groupby('firm_id')['tech_budget_pct'].shift(1).fillna(0)\n",
    "    df['ceo_exp_lag1'] = df.groupby('firm_id')['ceo_experience'].shift(1).fillna(0)\n",
    "    df['productivity_lag1'] = df.groupby('firm_id')['productivity_score'].shift(1).fillna(0)\n",
    "    \n",
    "    # Treatment history lags\n",
    "    df['treatment_lag1'] = df.groupby('firm_id')['digital_transformation_program'].shift(1).fillna(0)\n",
    "    df['treatment_lag2'] = df.groupby('firm_id')['digital_transformation_program'].shift(2).fillna(0)\n",
    "    \n",
    "    # Winsorize key features to handle outliers\n",
    "    features_to_winsorize = [\n",
    "        'rd_intensity', 'tech_budget_pct', 'productivity_score',\n",
    "        'employee_satisfaction', 'log_firm_size', 'rd_intensity_lag1'\n",
    "    ]\n",
    "    for feature in features_to_winsorize:\n",
    "        if feature in df.columns:\n",
    "            df[feature] = winsorize_series(df[feature])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ======================\n",
    "# MARGINAL STRUCTURAL MODELS (MSM) - Session 7\n",
    "# ======================\n",
    "\n",
    "def estimate_treatment_propensities(df):\n",
    "    \"\"\"Estimate treatment propensities for MSM/IPTW as taught in Session 7\"\"\"\n",
    "    print(\"Estimating treatment propensities for MSM...\")\n",
    "    \n",
    "    # Define confounders based on DAG (not empirical balance checking)\n",
    "    confounders = ['rd_intensity_lag1', 'tech_budget_lag1', 'ceo_exp_lag1',\n",
    "                   'productivity_lag1', 'treatment_lag1', 'treatment_lag2',\n",
    "                   'firm_age', 'log_firm_size']\n",
    "    \n",
    "    # Check for available confounders\n",
    "    available_conf = [c for c in confounders if c in df.columns]\n",
    "    print(f\"Using confounders: {available_conf}\")\n",
    "    \n",
    "    # Prepare data for propensity estimation\n",
    "    ps_data = df[available_conf + ['digital_transformation_program', 'year']].dropna()\n",
    "    \n",
    "    if ps_data.empty:\n",
    "        print(\"No valid data for propensity estimation\")\n",
    "        df['msm_weights'] = 1.0\n",
    "        return df\n",
    "    \n",
    "    # Add year fixed effects\n",
    "    year_dummies = pd.get_dummies(ps_data['year'], prefix='year')\n",
    "    X = pd.concat([ps_data[available_conf], year_dummies], axis=1)\n",
    "    y = ps_data['digital_transformation_program']\n",
    "    \n",
    "    # Fit logistic regression for P(D_it | past D, past confounders)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr.fit(X_scaled, y)\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    prob_treatment = lr.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate stabilized weights\n",
    "    marginal_prob = y.mean()\n",
    "    weights = np.where(y == 1, \n",
    "                      marginal_prob / np.maximum(prob_treatment, 0.01),\n",
    "                      (1 - marginal_prob) / np.maximum(1 - prob_treatment, 0.01))\n",
    "    \n",
    "    # Trim extreme weights\n",
    "    weights = np.clip(weights, 0.1, 10)\n",
    "    \n",
    "    # Add weights to dataframe\n",
    "    df_copy = df.copy()\n",
    "    weight_series = pd.Series(weights, index=ps_data.index)\n",
    "    df_copy['msm_weights'] = df_copy.index.map(weight_series).fillna(1.0)\n",
    "    \n",
    "    print(f\"‚úÖ MSM weights computed: mean={weights.mean():.3f}, min={weights.min():.3f}, max={weights.max():.3f}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def run_msm_analysis(df, outcome):\n",
    "    \"\"\"Run MSM analysis with IPTW as taught in Session 7\"\"\"\n",
    "    print(f\"Running MSM analysis for {outcome}...\")\n",
    "    \n",
    "    if 'msm_weights' not in df.columns:\n",
    "        df = estimate_treatment_propensities(df)\n",
    "    \n",
    "    # Set up panel data\n",
    "    panel_data = df.set_index(['firm_id', 'year'])\n",
    "    \n",
    "    # MSM specification: outcome ~ D_it + EntityEffects + TimeEffects\n",
    "    formula = f\"{outcome} ~ digital_transformation_program + EntityEffects + TimeEffects\"\n",
    "    \n",
    "    try:\n",
    "        model = PanelOLS.from_formula(formula, data=panel_data, weights=panel_data['msm_weights'])\n",
    "        results = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "        \n",
    "        return {\n",
    "            'ate': results.params['digital_transformation_program'],\n",
    "            'se': results.std_errors['digital_transformation_program'],\n",
    "            'pval': results.pvalues['digital_transformation_program'],\n",
    "            'model': results\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"MSM estimation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ======================\n",
    "# PARALLEL TRENDS TEST - Session 4 Approach\n",
    "# ======================\n",
    "\n",
    "def test_parallel_trends(df, outcome, pre_years=[2018, 2019]):\n",
    "    \"\"\"Test parallel trends using lead dummies with collinearity fix\"\"\"\n",
    "    print(f\"Testing parallel trends for {outcome}...\")\n",
    "    \n",
    "    # Create ever_treated indicator\n",
    "    ever_treated = df.groupby('firm_id')['digital_transformation_program'].max()\n",
    "    df = df.merge(ever_treated.rename('ever_treated'), left_on='firm_id', right_index=True)\n",
    "    \n",
    "    # Filter to pre-treatment years\n",
    "    pre_data = df[df['year'].isin(pre_years)].copy()\n",
    "    \n",
    "    if pre_data.empty:\n",
    "        print(\"No pre-treatment data available\")\n",
    "        return None\n",
    "    \n",
    "    # Create year dummies and interactions\n",
    "    pre_data['year_2019'] = (pre_data['year'] == 2019).astype(int)\n",
    "    pre_data['treated_x_2019'] = pre_data['ever_treated'] * pre_data['year_2019']\n",
    "    \n",
    "    # Run regression with collinearity fix\n",
    "    try:\n",
    "        panel_data = pre_data.set_index(['firm_id', 'year'])\n",
    "        \n",
    "        # SOLUTION: Combine both approaches\n",
    "        formula = f\"{outcome} ~ year_2019 + treated_x_2019 + EntityEffects\"\n",
    "        model = PanelOLS.from_formula(\n",
    "            formula, \n",
    "            data=panel_data,\n",
    "            drop_absorbed=True  # Option A: Auto-drop collinear terms\n",
    "        )\n",
    "        results = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "        \n",
    "        # Test if treated_x_2019 coefficient is significant\n",
    "        coef = results.params.get('treated_x_2019', np.nan)\n",
    "        pval = results.pvalues.get('treated_x_2019', np.nan)\n",
    "        \n",
    "        print(f\"Parallel trends test: coef={coef:.4f}, p-value={pval:.4f}\")\n",
    "        print(\"‚úÖ Parallel trends holds\" if pval > 0.05 else \"‚ö†Ô∏è Possible violation\")\n",
    "        \n",
    "        return {\n",
    "            'coefficient': coef,\n",
    "            'pvalue': pval,\n",
    "            'model': results\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Fallback to Option B if still failing\n",
    "        print(f\"Primary method failed ({e}), using simple DiD fallback\")\n",
    "        return simple_did_fallback(pre_data, outcome)\n",
    "\n",
    "def simple_did_fallback(df, outcome):\n",
    "    \"\"\"Option B: Two-period DiD without fixed effects\"\"\"\n",
    "    try:\n",
    "        did_formula = f\"{outcome} ~ ever_treated * year_2019\"\n",
    "        model = smf.ols(did_formula, data=df).fit()\n",
    "        coef = model.params.get('ever_treated:year_2019', np.nan)\n",
    "        pval = model.pvalues.get('ever_treated:year_2019', np.nan)\n",
    "        \n",
    "        print(f\"Simple DiD test: coef={coef:.4f}, p-value={pval:.4f}\")\n",
    "        return {\n",
    "            'coefficient': coef,\n",
    "            'pvalue': pval,\n",
    "            'model': model\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Fallback method failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ======================\n",
    "# SUBGROUP ANALYSIS - Session 5 Approach\n",
    "# ======================\n",
    "\n",
    "def run_basic_did(df, outcome):\n",
    "    \"\"\"Run basic difference-in-differences\"\"\"\n",
    "    try:\n",
    "        panel_data = df.set_index(['firm_id', 'year'])\n",
    "        formula = f\"{outcome} ~ digital_transformation_program + EntityEffects + TimeEffects\"\n",
    "        model = PanelOLS.from_formula(formula, data=panel_data)\n",
    "        results = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "        return results\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def run_subgroup_analysis(df, outcome, group_var):\n",
    "    \"\"\"Run subgroup analysis using filter-and-regress approach from Session 5\"\"\"\n",
    "    print(f\"Running subgroup analysis by {group_var}...\")\n",
    "    \n",
    "    if group_var not in df.columns:\n",
    "        print(f\"Group variable {group_var} not found\")\n",
    "        return []\n",
    "    \n",
    "    # Get unique groups\n",
    "    groups = df[group_var].cat.categories if hasattr(df[group_var], 'cat') else df[group_var].unique()\n",
    "    groups = [g for g in groups if pd.notna(g)]\n",
    "    \n",
    "    results = []\n",
    "    for group in groups:\n",
    "        subset = df[df[group_var] == group]\n",
    "        \n",
    "        if len(subset) < 50:  # Minimum sample size\n",
    "            print(f\"Group {group} too small ({len(subset)} observations)\")\n",
    "            continue\n",
    "        \n",
    "        # Run DiD on subset\n",
    "        did_results = run_basic_did(subset, outcome)\n",
    "        \n",
    "        if did_results and 'digital_transformation_program' in did_results.params.index:\n",
    "            results.append({\n",
    "                'group': group,\n",
    "                'ate': did_results.params['digital_transformation_program'],\n",
    "                'se': did_results.std_errors['digital_transformation_program'],\n",
    "                'pval': did_results.pvalues['digital_transformation_program'],\n",
    "                'n_obs': len(subset)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_moderation_analysis(df, outcome, moderator):\n",
    "    \"\"\"Run moderation analysis using interaction terms from Session 5\"\"\"\n",
    "    print(f\"Running moderation analysis with {moderator}...\")\n",
    "    \n",
    "    if moderator not in df.columns:\n",
    "        print(f\"Moderator {moderator} not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create interaction term\n",
    "        df_mod = df.copy()\n",
    "        df_mod['treatment_x_moderator'] = (df_mod['digital_transformation_program'] * \n",
    "                                         df_mod[moderator])\n",
    "        \n",
    "        # Set up panel\n",
    "        panel_data = df_mod.set_index(['firm_id', 'year'])\n",
    "        \n",
    "        # Run regression with interaction\n",
    "        formula = f\"{outcome} ~ digital_transformation_program + {moderator} + treatment_x_moderator + EntityEffects + TimeEffects\"\n",
    "        model = PanelOLS.from_formula(formula, data=panel_data)\n",
    "        results = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "        \n",
    "        return {\n",
    "            'main_effect': results.params.get('digital_transformation_program', np.nan),\n",
    "            'interaction_effect': results.params.get('treatment_x_moderator', np.nan),\n",
    "            'interaction_pval': results.pvalues.get('treatment_x_moderator', np.nan),\n",
    "            'model': results\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Moderation analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ======================\n",
    "# CAUSAL TREES - Session 5 Approach\n",
    "# ======================\n",
    "\n",
    "def standardize_outcomes(df, outcomes):\n",
    "    \"\"\"Standardize outcomes to z-scores for comparable effect sizes\"\"\"\n",
    "    df_std = df.copy()\n",
    "    for outcome in outcomes:\n",
    "        if outcome in df_std.columns:\n",
    "            mean = df_std[outcome].mean()\n",
    "            std = df_std[outcome].std()\n",
    "            df_std[outcome] = (df_std[outcome] - mean) / std\n",
    "    return df_std\n",
    "\n",
    "def run_causal_tree_analysis(df, outcome):\n",
    "    print(f\"Running causal tree analysis for '{outcome}'...\")\n",
    "\n",
    "    # Standardize outcome for comparable effect sizes\n",
    "    df = standardize_outcomes(df, [outcome])\n",
    "    \n",
    "    features = [\n",
    "        'log_firm_size', 'firm_age', 'ceo_experience',\n",
    "        'prior_tech_adoption', 'tech_budget_pct', 'rd_intensity'\n",
    "    ]\n",
    "    available_features = [f for f in features if f in df.columns]\n",
    "\n",
    "    if not available_features:\n",
    "        print(\"‚ùå No available features for causal tree analysis.\")\n",
    "        return None, None, None\n",
    "\n",
    "    required_columns = available_features + [outcome, 'digital_transformation_program']\n",
    "    analysis_data = df[required_columns].dropna()\n",
    "\n",
    "    if len(analysis_data) < 100:\n",
    "        print(f\"‚ùå Insufficient data: {len(analysis_data)} observations (min 100 required).\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Winsorize features to handle outliers\n",
    "    for feature in available_features:\n",
    "        analysis_data[feature] = winsorize_series(analysis_data[feature])\n",
    "\n",
    "    X = analysis_data[available_features].values\n",
    "    T = analysis_data['digital_transformation_program'].values\n",
    "    Y = analysis_data[outcome].values\n",
    "\n",
    "    try:\n",
    "        causal_tree = CausalTreeRegressor(\n",
    "            random_state=42,\n",
    "            min_samples_leaf=50,\n",
    "            max_depth=5,\n",
    "            min_impurity_decrease=-np.inf\n",
    "        )\n",
    "        causal_tree.fit(X=X, treatment=T, y=Y)\n",
    "\n",
    "        X_full = df[available_features].fillna(df[available_features].mean()).values\n",
    "        predictions = causal_tree.predict(X_full)\n",
    "\n",
    "        importances = causal_tree.feature_importances_\n",
    "        feature_importances = dict(zip(available_features, importances))\n",
    "\n",
    "        df_with_hte = df.copy()\n",
    "        df_with_hte['hte_prediction'] = predictions\n",
    "\n",
    "        print(\"‚úÖ Causal tree analysis completed successfully.\")\n",
    "        return causal_tree, df_with_hte, feature_importances\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Causal tree analysis failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "# ======================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ======================\n",
    "\n",
    "def plot_subgroup_results(subgroup_results, outcome, group_var):\n",
    "    \"\"\"Plot subgroup analysis results\"\"\"\n",
    "    if not subgroup_results:\n",
    "        return None\n",
    "    \n",
    "    df_results = pd.DataFrame(subgroup_results)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Error bar plot\n",
    "    ax.errorbar(range(len(df_results)), df_results['ate'], \n",
    "               yerr=1.96 * df_results['se'], fmt='o-', capsize=5, markersize=8)\n",
    "    \n",
    "    # Add reference line at 0\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xticks(range(len(df_results)))\n",
    "    ax.set_xticklabels(df_results['group'], rotation=45)\n",
    "    ax.set_ylabel('Average Treatment Effect (SD Units)')\n",
    "    ax.set_title(f'Treatment Effects by {group_var.replace(\"_\", \" \").title()}\\n{outcome.replace(\"_\", \" \").title()}')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_hte_distribution(df, outcome):\n",
    "    \"\"\"Plot HTE distribution\"\"\"\n",
    "    if 'hte_prediction' not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    hte_values = df['hte_prediction'].dropna()\n",
    "    if len(hte_values) == 0:\n",
    "        return None\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Distribution plot\n",
    "    ax1.hist(hte_values, bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(hte_values.mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {hte_values.mean():.3f}')\n",
    "    ax1.set_xlabel('Predicted Treatment Effect (SD Units)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Distribution of Heterogeneous Treatment Effects')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # HTE by firm size\n",
    "    if 'log_firm_size' in df.columns:\n",
    "        plot_data = df.dropna(subset=['hte_prediction', 'log_firm_size'])\n",
    "        if len(plot_data) > 0:\n",
    "            plot_data['size_quartile'] = pd.qcut(plot_data['log_firm_size'], 4, \n",
    "                                               labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "            quartile_stats = plot_data.groupby('size_quartile')['hte_prediction'].agg(['mean', 'std'])\n",
    "            \n",
    "            ax2.bar(quartile_stats.index, quartile_stats['mean'], \n",
    "                   yerr=quartile_stats['std'], capsize=5, alpha=0.7)\n",
    "            ax2.set_xlabel('Firm Size Quartile')\n",
    "            ax2.set_ylabel('Mean Treatment Effect (SD Units)')\n",
    "            ax2.set_title('HTE by Firm Size Quartile')\n",
    "            ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_feature_hte(df, feature, outcome):\n",
    "    \"\"\"Plot HTE against a specific feature\"\"\"\n",
    "    if 'hte_prediction' not in df.columns or feature not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    plot_data = df.dropna(subset=['hte_prediction', feature])\n",
    "    if len(plot_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Add jitter to reduce overplotting\n",
    "    jitter = 0.1 * (plot_data[feature].max() - plot_data[feature].min())\n",
    "    x_jitter = plot_data[feature] + np.random.uniform(-jitter, jitter, size=len(plot_data))\n",
    "    \n",
    "    ax.scatter(x_jitter, plot_data['hte_prediction'], alpha=0.5)\n",
    "    \n",
    "    # Add regression line with confidence interval\n",
    "    try:\n",
    "        sns.regplot(x=feature, y='hte_prediction', data=plot_data, \n",
    "                   scatter=False, color='red', ax=ax,\n",
    "                   ci=95, line_kws={'lw': 2})\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Predicted Treatment Effect (SD Units)')\n",
    "    ax.set_title(f'Treatment Effect Heterogeneity by {feature.replace(\"_\", \" \").title()}')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_ate_comparison(ate_results):\n",
    "    \"\"\"Plot ATE comparison across outcomes in standardized units\"\"\"\n",
    "    outcomes = list(ate_results.keys())\n",
    "    ates = [ate_results[o]['ate'] for o in outcomes]\n",
    "    ci_lower = [ate_results[o]['ate'] - 1.96 * ate_results[o]['se'] for o in outcomes]\n",
    "    ci_upper = [ate_results[o]['ate'] + 1.96 * ate_results[o]['se'] for o in outcomes]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.errorbar(outcomes, ates, yerr=[ates[i]-ci_lower[i] for i in range(len(ates))], \n",
    "               fmt='o', capsize=5)\n",
    "    ax.axhline(0, color='red', linestyle='--')\n",
    "    ax.set_title('Average Treatment Effects Across Outcomes (SD Units)')\n",
    "    ax.set_ylabel('ATE with 95% CI (Standardized)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ======================\n",
    "# MAIN ANALYSIS PIPELINE\n",
    "# ======================\n",
    "\n",
    "def main_analysis():\n",
    "    \"\"\"Run the complete course-aligned causal analysis pipeline\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"COURSE-ALIGNED CAUSAL ANALYSIS PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_and_validate_data()\n",
    "    if df is None: \n",
    "        return None\n",
    "    \n",
    "    df = preprocess_data(df)\n",
    "    if df is None: \n",
    "        return None\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['firm_id', 'year', 'digital_transformation_program']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
    "        return None\n",
    "    \n",
    "    # Standardize outcomes for comparable effect sizes\n",
    "    outcomes_to_standardize = [col for col in ['productivity_score', 'employee_satisfaction'] \n",
    "                              if col in df.columns]\n",
    "    df = standardize_outcomes(df, outcomes_to_standardize)\n",
    "    \n",
    "    # Estimate MSM weights\n",
    "    df = estimate_treatment_propensities(df)\n",
    "    \n",
    "    # Determine available outcomes\n",
    "    potential_outcomes = ['productivity_score', 'employee_satisfaction']\n",
    "    available_outcomes = [outcome for outcome in potential_outcomes if outcome in df.columns]\n",
    "    \n",
    "    if not available_outcomes:\n",
    "        print(\"‚ùå No valid outcome variables found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìä Analyzing outcomes: {available_outcomes}\")\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = {}\n",
    "    ate_results = {}\n",
    "    \n",
    "    for outcome in available_outcomes:\n",
    "        print(f\"\\n{'='*20} ANALYZING: {outcome.upper()} {'='*20}\")\n",
    "        \n",
    "        outcome_results = {}\n",
    "        \n",
    "        # 1. MSM Analysis (Session 7)\n",
    "        print(\"\\n1Ô∏è‚É£ Running MSM/IPTW Analysis...\")\n",
    "        msm_results = run_msm_analysis(df, outcome)\n",
    "        outcome_results['msm'] = msm_results\n",
    "        \n",
    "        if msm_results:\n",
    "            ate_results[outcome] = {\n",
    "                'ate': msm_results['ate'],\n",
    "                'se': msm_results['se']\n",
    "            }\n",
    "        \n",
    "        # 2. Parallel Trends Test (Session 4)\n",
    "        print(\"\\n2Ô∏è‚É£ Testing Parallel Trends...\")\n",
    "        pt_results = test_parallel_trends(df, outcome)\n",
    "        outcome_results['parallel_trends'] = pt_results\n",
    "        \n",
    "        # 3. Subgroup Analysis (Session 5)\n",
    "        print(\"\\n3Ô∏è‚É£ Running Subgroup Analysis...\")\n",
    "        if 'firm_size_bin' in df.columns:\n",
    "            size_subgroups = run_subgroup_analysis(df, outcome, 'firm_size_bin')\n",
    "            outcome_results['subgroup_size'] = size_subgroups\n",
    "        \n",
    "        if 'prior_tech_adoption' in df.columns:\n",
    "            tech_subgroups = run_subgroup_analysis(df, outcome, 'prior_tech_adoption')\n",
    "            outcome_results['subgroup_tech'] = tech_subgroups\n",
    "        \n",
    "        # 4. Moderation Analysis (Session 5)\n",
    "        print(\"\\n4Ô∏è‚É£ Running Moderation Analysis...\")\n",
    "        if 'log_firm_size' in df.columns:\n",
    "            mod_results = run_moderation_analysis(df, outcome, 'log_firm_size')\n",
    "            outcome_results['moderation'] = mod_results\n",
    "        \n",
    "        # 5. Causal Tree Analysis (Session 5)\n",
    "        print(\"\\n5Ô∏è‚É£ Running Causal Tree Analysis...\")\n",
    "        tree_model, df_with_hte, feature_importances = run_causal_tree_analysis(df, outcome)\n",
    "        outcome_results['causal_tree'] = tree_model\n",
    "        outcome_results['hte_data'] = df_with_hte\n",
    "        outcome_results['feature_importances'] = feature_importances\n",
    "        \n",
    "        # Generate visualizations\n",
    "        try:\n",
    "            # Subgroup plots\n",
    "            if 'subgroup_size' in outcome_results and outcome_results['subgroup_size']:\n",
    "                fig = plot_subgroup_results(outcome_results['subgroup_size'], outcome, 'firm_size_bin')\n",
    "                if fig:\n",
    "                    fig.savefig(f'{outcome}_subgroup_size.png', dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "            \n",
    "            if 'subgroup_tech' in outcome_results and outcome_results['subgroup_tech']:\n",
    "                fig = plot_subgroup_results(outcome_results['subgroup_tech'], outcome, 'prior_tech_adoption')\n",
    "                if fig:\n",
    "                    fig.savefig(f'{outcome}_subgroup_tech.png', dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "            \n",
    "            # HTE plots\n",
    "            if df_with_hte is not None:\n",
    "                fig = plot_hte_distribution(df_with_hte, outcome)\n",
    "                if fig:\n",
    "                    fig.savefig(f'{outcome}_hte_distribution.png', dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "                \n",
    "                if feature_importances:\n",
    "                    top_feature = max(feature_importances, key=feature_importances.get)\n",
    "                    fig = plot_feature_hte(df_with_hte, top_feature, outcome)\n",
    "                    if fig:\n",
    "                        fig.savefig(f'{outcome}_hte_{top_feature}.png', dpi=300, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Visualization error for {outcome}: {e}\")\n",
    "        \n",
    "        all_results[outcome] = outcome_results\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    print(\"\\nüìù Generating comprehensive report...\")\n",
    "    report_lines = []\n",
    "    report_lines.append(\"COURSE-ALIGNED CAUSAL ANALYSIS RESULTS\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "    report_lines.append(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report_lines.append(f\"Total Observations: {len(df)}\")\n",
    "    report_lines.append(f\"Unique Firms: {df['firm_id'].nunique()}\")\n",
    "    report_lines.append(f\"Time Periods: {sorted(df['year'].unique())}\")\n",
    "    \n",
    "    # ATE comparison\n",
    "    report_lines.append(\"\\n\" + \"=\"*60)\n",
    "    report_lines.append(\"AVERAGE TREATMENT EFFECTS (ATE) ACROSS OUTCOMES\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "    for outcome, res in ate_results.items():\n",
    "        report_lines.append(f\"\\nüìä {outcome.upper()}:\")\n",
    "        report_lines.append(f\"  ATE: {res['ate']:.4f} SD Units\")\n",
    "        report_lines.append(f\"  95% CI: [{res['ate']-1.96*res['se']:.4f}, {res['ate']+1.96*res['se']:.4f}]\")\n",
    "        report_lines.append(f\"  Standard Error: ¬±{res['se']:.4f}\")\n",
    "    \n",
    "    # Detailed results per outcome\n",
    "    for outcome_name, results in all_results.items():\n",
    "        report_lines.append(f\"\\n{'='*20} {outcome_name.upper()} DETAILED RESULTS {'='*20}\")\n",
    "        \n",
    "        # MSM Results\n",
    "        if results.get('msm'):\n",
    "            msm = results['msm']\n",
    "            report_lines.append(f\"\\nüìä MSM-IPTW RESULTS:\")\n",
    "            report_lines.append(f\"   Average Treatment Effect: {msm['ate']:.4f} SD Units\")\n",
    "            report_lines.append(f\"   Standard Error: {msm['se']:.4f}\")\n",
    "            report_lines.append(f\"   P-value: {msm['pval']:.4f}\")\n",
    "            report_lines.append(f\"   Significance: {'***' if msm['pval'] < 0.01 else '**' if msm['pval'] < 0.05 else '*' if msm['pval'] < 0.1 else 'Not significant'}\")\n",
    "        \n",
    "        # Parallel Trends\n",
    "        if results.get('parallel_trends'):\n",
    "            pt = results['parallel_trends']\n",
    "            report_lines.append(f\"\\nüìà PARALLEL TRENDS TEST:\")\n",
    "            report_lines.append(f\"   Lead coefficient: {pt['coefficient']:.4f}\")\n",
    "            report_lines.append(f\"   P-value: {pt['pvalue']:.4f}\")\n",
    "            report_lines.append(f\"   Assessment: {'‚úÖ Assumption likely holds' if pt['pvalue'] > 0.05 else '‚ö†Ô∏è Potential violation'}\")\n",
    "        \n",
    "        # Subgroup Results\n",
    "        for subgroup_type in ['subgroup_size', 'subgroup_tech']:\n",
    "            if results.get(subgroup_type):\n",
    "                group_name = 'Firm Size' if 'size' in subgroup_type else 'Tech Adoption'\n",
    "                report_lines.append(f\"\\nüè¢ SUBGROUP ANALYSIS ({group_name}):\")\n",
    "                for result in results[subgroup_type]:\n",
    "                    sig = '***' if result['pval'] < 0.01 else '**' if result['pval'] < 0.05 else '*' if result['pval'] < 0.1 else ''\n",
    "                    report_lines.append(f\"   {result['group']}: ATE={result['ate']:.4f} (SE={result['se']:.4f}) {sig}\")\n",
    "        \n",
    "        # Moderation Results\n",
    "        if results.get('moderation'):\n",
    "            mod = results['moderation']\n",
    "            report_lines.append(f\"\\nüîÑ MODERATION ANALYSIS:\")\n",
    "            report_lines.append(f\"   Main Effect: {mod['main_effect']:.4f}\")\n",
    "            report_lines.append(f\"   Interaction Effect: {mod['interaction_effect']:.4f}\")\n",
    "            report_lines.append(f\"   Interaction P-value: {mod['interaction_pval']:.4f}\")\n",
    "        \n",
    "        # HTE Results\n",
    "        hte_data = results.get('hte_data')\n",
    "        if isinstance(hte_data, pd.DataFrame) and 'hte_prediction' in hte_data.columns:\n",
    "            hte_vals = hte_data['hte_prediction'].dropna()\n",
    "            if len(hte_vals) > 0:\n",
    "                report_lines.append(f\"\\nüå≥ HETEROGENEOUS TREATMENT EFFECTS:\")\n",
    "                report_lines.append(f\"   Mean HTE: {hte_vals.mean():.4f} SD Units\")\n",
    "                report_lines.append(f\"   Median HTE: {hte_vals.median():.4f}\")\n",
    "                report_lines.append(f\"   Standard Deviation: {hte_vals.std():.4f}\")\n",
    "                report_lines.append(f\"   Range: [{hte_vals.min():.4f}, {hte_vals.max():.4f}]\")\n",
    "                report_lines.append(f\"   % Positive Effects: {(hte_vals > 0).mean()*100:.1f}%\")\n",
    "        \n",
    "        # Feature Importances\n",
    "        feat_importances = results.get('feature_importances')\n",
    "        if feat_importances:\n",
    "            report_lines.append(f\"\\nüîç FEATURE IMPORTANCE FOR HTE:\")\n",
    "            for feature, importance in sorted(feat_importances.items(), key=lambda x: x[1], reverse=True):\n",
    "                report_lines.append(f\"   {feature}: {importance:.4f}\")\n",
    "    \n",
    "    # Save report\n",
    "    try:\n",
    "        with open('course_aligned_causal_results.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(report_lines))\n",
    "        \n",
    "        # Create ATE comparison plot\n",
    "        if ate_results:\n",
    "            fig = plot_ate_comparison(ate_results)\n",
    "            if fig:\n",
    "                fig.savefig('ate_comparison.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "        \n",
    "        print(\"‚úÖ Analysis complete! Results saved:\")\n",
    "        print(\"  - course_aligned_causal_results.txt\")\n",
    "        print(\"  - ate_comparison.png\")\n",
    "        print(\"  - Outcome-specific visualizations\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error saving report: {e}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results = main_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
